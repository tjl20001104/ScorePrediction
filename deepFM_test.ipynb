{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(nn.Module):\n",
    "    def __init__(self, num_user, num_book, latent_dim, n_hidden_1, n_hidden_2, n_output_1, n_output_2):\n",
    "        \"\"\"\n",
    "        latent_dim: 各个离散特征隐向量的维度\n",
    "        input_shape: 这个最后离散特征embedding之后的拼接和dense拼接的总特征个数\n",
    "        feature_1_user: 用户带bias的embedding vector 1 x latent_dim\n",
    "        feature_1_book: 书带bias的embedding vector 1 x latent_dim\n",
    "        feature_high_user: 用户高阶特征向量 1 x n_output_1\n",
    "        feature_high_book: 书高阶特征向量 1 x n_output_1\n",
    "        \"\"\"\n",
    "        super(FM, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # 定义三个矩阵， 一个是全局偏置，一个是一阶权重矩阵， 一个是二阶交叉矩阵，注意这里的参数由于是可学习参数，需要用nn.Parameter进行定义\n",
    "        self.bias_user = nn.Parameter(torch.ones([1, latent_dim]))\n",
    "        self.bias_book = nn.Parameter(torch.ones([1, latent_dim]))\n",
    "        self.emb_user = nn.Embedding(num_user,latent_dim)\n",
    "        self.emb_book = nn.Embedding(num_book,latent_dim)\n",
    "        self.network_user = nn.Sequential(\n",
    "            nn.Linear(latent_dim, n_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden_1, n_output_1)\n",
    "        )\n",
    "        self.network_book = nn.Sequential(\n",
    "            nn.Linear(latent_dim, n_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden_1, n_output_1)\n",
    "        )\n",
    "        self.net_similatiry = nn.Sequential(\n",
    "            nn.Linear(2*n_output_1 + 2*latent_dim, n_hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden_2, n_output_2)\n",
    "        )\n",
    " \n",
    "    def forward(self, inputs):\n",
    "        feature_1_user = self.emb_user(inputs[0]) + self.bias_user\n",
    "        feature_1_book = self.emb_book(inputs[0]) + self.bias_book\n",
    "\n",
    "        feature_high_user = self.network_user(feature_1_user)\n",
    "        feature_high_book = self.network_book(feature_1_book)\n",
    "\n",
    "        feature_all = torch.cat([feature_1_user, feature_1_book, feature_high_user, feature_high_book],1)\n",
    "        score = self.net_similatiry(feature_all)\n",
    "        \n",
    "        if n_output_2 == 1:\n",
    "            print(score)\n",
    "            return score\n",
    "        else:\n",
    "            score = F.softmax(score, dim=1)\n",
    "            return score.argmax(dim=1, keepdim=False) + 0. # Change texsor to float type\n",
    "\n",
    "    def params(self):\n",
    "        params = [self.bias_user,\n",
    "                  self.bias_book,\n",
    "                  self.emb_user.parameters(),\n",
    "                  self.emb_book.parameters(),\n",
    "                  self.network_user.parameters(),\n",
    "                  self.network_book.parameters(),\n",
    "                  self.net_similatiry.parameters()]\n",
    "        return filter(lambda p: p.requires_grad, chain(*params))\n",
    "\n",
    "\n",
    "class Ratingdataset(Dataset):\n",
    "    def __init__(self, data_path, user2idx, book2idx):\n",
    "        df = pd.read_csv(data_path, header=None, index_col=None)\n",
    "        df = df.drop_duplicates()\n",
    "        # df = df.drop(df[df[2] == 0].index) # New\n",
    "        self.user2idx = user2idx\n",
    "        self.book2idx = book2idx\n",
    "        self.data = df.values\n",
    "        self._len = df.shape[0]\n",
    "\n",
    "    def __getitem__(self, id_index):\n",
    "        user_idx = self.user2idx[self.data[id_index,0]]\n",
    "        book_idx = self.book2idx[self.data[id_index,1]]\n",
    "        rate = self.data[id_index,2]\n",
    "        return user_idx,book_idx,rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"Ratings.csv\",header=0,index_col=None)\n",
    "df_all = df_all.drop_duplicates()\n",
    "user_all = df_all.iloc[:,0].unique().tolist()\n",
    "user2idx = {}\n",
    "book_all = df_all.iloc[:,1].unique().tolist()\n",
    "book2idx = {}\n",
    "for i in range(len(user_all)):\n",
    "    user2idx[user_all[i]] = i\n",
    "for i in range(len(book_all)):\n",
    "    book2idx[book_all[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "batch_size = 1000\n",
    "lr = 1e-4\n",
    "latent_dim = 10\n",
    "n_hidden_1 = 50\n",
    "n_hidden_2 = 30\n",
    "n_output_1 = 25\n",
    "# n_output_2 = 1\n",
    "n_output_2 = 11\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FM(len(user2idx), len(book2idx), latent_dim=latent_dim, \n",
    "        n_hidden_1=n_hidden_1, n_hidden_2=n_hidden_2, \n",
    "        n_output_1=n_output_1, n_output_2=n_output_2).to(device)\n",
    "\n",
    "\n",
    "dataset_train = Ratingdataset(\"train_ratings.csv\", user2idx, book2idx)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_test = Ratingdataset(\"test_ratings.csv\", user2idx, book2idx)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "FM                                       20\n",
       "├─Embedding: 1-1                         955,130\n",
       "├─Embedding: 1-2                         3,221,010\n",
       "├─Sequential: 1-3                        --\n",
       "│    └─Linear: 2-1                       550\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Linear: 2-3                       1,275\n",
       "├─Sequential: 1-4                        --\n",
       "│    └─Linear: 2-4                       550\n",
       "│    └─ReLU: 2-5                         --\n",
       "│    └─Linear: 2-6                       1,275\n",
       "├─Sequential: 1-5                        --\n",
       "│    └─Linear: 2-7                       2,130\n",
       "│    └─ReLU: 2-8                         --\n",
       "│    └─Linear: 2-9                       341\n",
       "=================================================================\n",
       "Total params: 4,182,281\n",
       "Trainable params: 4,182,281\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING USING DEVICE cpu\n",
      "loss=1440.9405517578125\n",
      "loss=1311.1566162109375\n",
      "loss=1196.114990234375\n",
      "loss=1202.2314453125\n",
      "loss=1116.832763671875\n",
      "loss=965.256103515625\n",
      "loss=882.12109375\n",
      "loss=820.702392578125\n",
      "loss=737.8413696289062\n",
      "loss=676.1389770507812\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(\"START TRAINING USING DEVICE {}\".format(device))\n",
    "\n",
    "for it in tqdm(range(0,epoch), disable=None):\n",
    "    for index,item in enumerate(dataloader_train):\n",
    "        inputs = [item[i].to(device) for i in range(2)]\n",
    "        ground_truth = item[2].to(device)\n",
    "        preds = model(inputs)\n",
    "        l2_reg = torch.tensor(0.).to(device)\n",
    "        for param in model.params():\n",
    "            l2_reg += torch.norm(param)\n",
    "        loss = torch.norm(preds - ground_truth, p=2) + alpha * l2_reg\n",
    "        # loss = torch.SUM((ground_truth / 10) * np.log(preds) + (1 - ground_truth / 10) * np.log(1 - preds))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"loss={}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4929)\n"
     ]
    }
   ],
   "source": [
    "RMSE = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for index,item in enumerate(dataloader_test):\n",
    "        inputs = [item[i].to(device) for i in range(2)]\n",
    "        preds = model(inputs)\n",
    "        ground_truth = item[2].to(device)\n",
    "        RMSE += torch.sum(torch.square(preds - ground_truth))\n",
    "        RMSE = RMSE/dataset_test.__len__()\n",
    "    print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9.])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练记录\n",
    "\n",
    "使用Adam损失函数训练，RMSE=36.8019\n",
    "\n",
    "使用Adam损失函数训练，使用Softmax作为激活函数,对1-10的分数预测，RMSE=0.1794， epoch=10, latent=50\n",
    "\n",
    "使用Adam损失函数训练，使用Softmax作为激活函数，对0-10的分数预测，RMSE=0.1336， epoch=10, latent=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae51ee3d492f24e83e77a52eb34bf16365894f8747390aa8e17995579dedf394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
